from flask import Flask, request, Response, stream_with_context, jsonify
from flask_cors import CORS
import requests
import urllib.parse
from bs4 import BeautifulSoup
import random
import time
import re

app = Flask(__name__)
CORS(app)  # Enged√©lyezi a CORS-t minden domain sz√°m√°ra

OLLAMA_URL = "http://localhost:11434"

# ==============================================================================
# DUCKDUCKGO KERES≈ê FUNKCI√ìK
# ==============================================================================

def _extract_final_url(link_element):
    """URL kinyer√©se a link elemb≈ël (DDG redirect kezel√©se)."""
    try:
        href = link_element.get("href")
        if not href:
            return None
            
        if '/l/?uddg=' in href or 'duckduckgo.com/l/' in href:
            if href.startswith('//'):
                href = 'https:' + href
                
            parsed_url = urllib.parse.urlparse(href)
            query_params = urllib.parse.parse_qs(parsed_url.query)
            
            if 'uddg' in query_params and query_params['uddg']:
                target_url_encoded = query_params['uddg'][0]
                return urllib.parse.unquote(target_url_encoded)
        
        if href.startswith('http'):
            return href
            
        return None
        
    except Exception:
        return None

def is_relevant_url(url, query):
   
    """Ellen≈ërzi, hogy egy URL relev√°ns-e a keres√©si lek√©rdez√©shez √©s nem egy trivi√°lis kiz√°rt form√°tum."""
    if not url or not isinstance(url, str):
        return False
        
    # Kiz√°rt mint√°k (zajforr√°sok)
    excluded_patterns = [
       'duckduckgo.com', 'google.com/search', 'bing.com/search', 
       '/Special:', '/Category:', 'translate.google', 'youtube.com',
       '.pdf', '.jpg', '.png', '.gif', '.zip', '.rar' # F√°jlt√≠pusok
    ]
    
    for pattern in excluded_patterns:
        if pattern in url.lower():
            return False
    
    try:
        parsed = urllib.parse.urlparse(url)
        if not parsed.netloc:
            return False
    except:
        return False
        
    return True

def search_duckduckgo_links(query, max_results=15):
    """Jav√≠tott DuckDuckGo keres√©s t√∂bb fallback opci√≥val, csak linkeket ad vissza."""
    links = []
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    })

    # 1. API pr√≥b√°lkoz√°s (ha el√©rhet≈ë, gyorsabb √©s stabilabb)
    try:
        api_url = "https://api.duckduckgo.com/"
        params = {
            'q': query,
            'format': 'json',
            'no_html': '1',
            'skip_disambig': '1'
        }
        
        response = session.get(api_url, params=params, timeout=5)
        response.raise_for_status()
        data = response.json()
        
        for topic in data.get('RelatedTopics', []):
            if isinstance(topic, dict) and 'FirstURL' in topic:
                url = topic['FirstURL']
                if is_relevant_url(url, query):
                    links.append(url)
                    if len(links) >= max_results:
                        break
        
    except Exception:
       pass
    
    # 2. HTML scraping (ha az API nem adott elegend≈ë tal√°latot)
    if len(links) < max_results:
        try:
            encoded_query = urllib.parse.quote_plus(query)
            ddg_url = f"https://duckduckgo.com/html/?q={encoded_query}"
            
            # K√©sleltet√©s a robot detekt√°l√°s elker√ºl√©se √©rdek√©ben
            time.sleep(random.uniform(1, 3))
            
            response = session.get(ddg_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, "html.parser")
            selectors = ["a.result__a", ".result .result__title a"]
            
            for selector in selectors:
                results = soup.select(selector)
                if results:
                    for r in results[:max_results - len(links)]:
                        final_url = _extract_final_url(r)
                        if final_url and is_relevant_url(final_url, query):
                            if final_url not in links:
                                links.append(final_url)
                                if len(links) >= max_results:
                                    break
                    break
                    
        except Exception:
           pass
    
    return list(dict.fromkeys(links))  # Duplik√°tumok elt√°vol√≠t√°sa

def fetch_content_snippet(url):
    """
    Jav√≠tott funkci√≥: Let√∂lti a tartalom egy r√©szlet√©t a megadott URL-r≈ël, 
    a f≈ë tartalomra f√≥kusz√°lva (pl. article, main).
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; OllamaAIAgent/1.0; +https://example.com)',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'hu-HU,hu;q=0.8,en-US;q=0.5,en;q=0.3',
        }
        
        # M√≥dos√≠t√°s: Szigor√∫bb timeout a let√∂lt√©si hib√°k cs√∂kkent√©s√©re
        response = requests.get(url, headers=headers, timeout=5) 
        response.raise_for_status()
        
        # M√≥dos√≠t√°s: K√≥dol√°s kezel√©se
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, "html.parser")
        
        # C√≠m kinyer√©se
        title = soup.title.string if soup.title else url
        
        # √öJ LOGIKA: F≈ë tartalom kinyer√©se, a zaj cs√∂kkent√©s√©re.
        # Megpr√≥b√°ljuk megtal√°lni a legrelev√°nsabb HTML elemeket.
        content_selectors = [
            'article', 
            'main', 
            'div[role="main"]',
            '.post-content', # Gyakori blog elem
            '#content',
            '#main'
        ]
        
        main_content_element = None
        for selector in content_selectors:
            main_content_element = soup.select_one(selector)
            if main_content_element:
                break
        
        # Ha nem tal√°ltunk specifikus elemet, visszat√©r√ºnk a body-hoz.
        if not main_content_element:
            main_content_element = soup.body
            
        page_text = main_content_element.get_text() if main_content_element else soup.get_text()
        
        # Tiszt√≠t√°s √©s korl√°toz√°s
        # Elt√°vol√≠tjuk a felesleges sz√≥k√∂z√∂ket/√∫j sorokat
        cleaned_content = ' '.join(page_text.split()).strip()
        
        # M√≥dos√≠t√°s: TARTALOM HOSSZ√ÅNAK N√ñVEL√âSE 3000 -> 5000 karakterre
        content_limit = 5000 
        
        return {
            "url": url,
            "title": title.strip() if title else url,
            "content": cleaned_content[:content_limit] 
        }

    except requests.exceptions.Timeout:
         return {
            "url": url,
            "title": url,
            "content": "Tartalom let√∂lt√©se sikertelen: Id≈ët√∫ll√©p√©s."
        }
    except requests.exceptions.RequestException as e:
         return {
            "url": url,
            "title": url,
            "content": f"Tartalom let√∂lt√©se sikertelen: HTTP hiba ({e.response.status_code if e.response else 'ismeretlen'})."
        }
    except Exception:
        # √Åltal√°nos hiba
        return {
            "url": url,
            "title": url,
            "content": "Tartalom let√∂lt√©se sikertelen: Ismeretlen hiba."
        }


# ==============================================================================
# √öJ PROXY √öTVONAL a DuckDuckGo-hoz
# ==============================================================================

@app.route('/api/duckduckgo', methods=['POST'])
def duckduckgo_search_endpoint():
    """Kezeli a kliens DuckDuckGo keres√©si k√©r√©s√©t, let√∂lti a linkeket √©s a tartalmukat."""
    try:
        data = request.get_json()
        query = data.get('query')
        # A kliens k√©rheti, hogy csak az els≈ë 3 tal√°latot t√∂ltse le
        max_results = int(data.get('max_results', 3))
        
        if not query:
            return jsonify({"error": "Hi√°nyz√≥ 'query' param√©ter."}), 400

        # 1. Relev√°ns linkek keres√©se
        links = search_duckduckgo_links(query, max_results=15) 
        
        results_with_content = []
        
        # 2. Az els≈ë 'max_results' link tartalm√°nak let√∂lt√©se
        for url in links[:max_results]:
            # K√©sleltet√©s a weboldalak let√∂lt√©se k√∂z√∂tt, hogy ne legy√ºnk t√∫l agressz√≠vak
            time.sleep(random.uniform(0.5, 1.5))
            content_data = fetch_content_snippet(url)
            results_with_content.append(content_data)

        return jsonify(results_with_content), 200

    except Exception as e:
        # √Åltal√°nos hiba
        return jsonify({"error": f"Bels≈ë szerver hiba a DDG keres√©s sor√°n: {str(e)}"}), 500


# ==============================================================================
# Eredeti Ollama Proxy √ötvonalak
# ==============================================================================

@app.route('/api/<path:path>', methods=['GET', 'POST', 'PUT', 'DELETE'])
def proxy(path):
    """Proxy az Ollama API fel√©"""
    url = f"{OLLAMA_URL}/api/{path}"
    
    if request.method == 'POST':
        def generate():
            with requests.post(
                url, 
                json=request.get_json(),
                stream=True,
                headers={'Content-Type': 'application/json'}
            ) as resp:
                for chunk in resp.iter_content(chunk_size=1024):
                    if chunk:
                        yield chunk
        
        return Response(
            stream_with_context(generate()),
            content_type='application/x-ndjson'
        )
    
    elif request.method == 'GET':
        resp = requests.get(url)
        return Response(resp.content, content_type=resp.headers.get('content-type'))
    
    return Response("Method not allowed", status=405)

if __name__ == '__main__':
    print("=" * 60)
    print("üöÄ Ollama CORS Proxy szerver ind√≠t√°sa...")
    print("=" * 60)
    print("‚úì Proxy el√©rhet≈ë: http://localhost:5000")
    print("‚úì Ollama API: http://localhost:11434")
    print("‚úì √öJ √∫tvonal: http://localhost:5000/api/duckduckgo")
    print("=" * 60)
    print("K√∂telez≈ë csomagok: pip install flask flask-cors requests beautifulsoup4")
    print("=" * 60)
    app.run(host='0.0.0.0', port=5000, debug=False)
